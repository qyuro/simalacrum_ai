SIMULACRUM AI — Симуляция автономных агентов

Краткое описание
Мы разработали интерактивную симуляцию социальных взаимодействий между AI-агентами, которая использует LLM для генерации действий агентов. Каждый агент в нашей системе имеет личность, роль, настроение, память и отношения с другими агентами.

Основная идея
Мы создали виртуальный мир, где несколько агентов с уникальными личностями принимают решения на основе контекста, памяти и отношений. Решения генерируются внешними моделями через сервисы в `services/`.

Архитектура и стек технологий
- Frontend: React 19 + TypeScript + Vite
- Визуализация: D3.js (граф отношений в `components/RelationshipGraph.tsx`)
- AI Backend: вызовы к LLM через `services/` — `geminiService.ts`, `openaiService.ts`, `ollamaService.ts`
- Стили: Tailwind CSS (в проекте используется, подключение через конфигурации/стили)
- Ключевые файлы и компоненты:
  - `App.tsx` — основная логика симуляции и тики
  - `components/AgentCard.tsx`, `AgentInspector.tsx`, `ControlPanel.tsx`, `EventLog.tsx`, `InspectorPanel.tsx`, `RelationshipGraph.tsx`
  - `types.ts`, `constants.ts` — типы и начальные агенты

Ключевая механика (совпадает с кодом)
- Агент: `id`, `name`, `avatar`, `role`, `personality`, `mood`, `moodIntensity`, `memories` (до 20), `relationships` (affinity -100..100), `isThinking`.
- Доступные действия: `TALK`, `THINK`, `WORK`, `REST`, `MOVE`.
- Симуляция: тики с настраиваемой скоростью (1x–10x). На каждый тик случайный свободный агент решает действие через `generateAgentAction` из `services/`.
- Mutex: в `App.tsx` используется `processingAgentRef` — мы гарантируем, что только один агент действует одновременно.
- Сохранение состояния в рантайме: мы сохраняем последние 100 логов и до 20 воспоминаний на агента (как в коде).

Цели защиты проекта (Project Protection Plan)
1) Защита секретов и API-ключей
   - Мы никогда не храним API-ключи в репозитории. Используем переменные окружения и файлы `.env`/`.env.local`, добавленные в `.gitignore`.
   - В проекте `services/openaiService.ts`, `services/geminiService.ts`, `services/ollamaService.ts` читают ключи из `import.meta.env` или `process.env` — мы оставляем ключи только в окружении развертывания.
   - Мы не коммитим файлы с ключами; в CI/CD заменяем ключи секретами платформы.
   - Для локальной работы мы загружаем ключи из отдельного локального файла (например `.env.local`) — этот файл не хранится в репозитории.

2) Проксирование LLM-запросов (рекомендуется)
   - Мы не делаем прямых вызовов к OpenAI/Gemini из браузера в продакшене (в `openaiService.ts` есть соответствующее предупреждение). Вместо этого:
     - Мы разворачиваем backend-proxy (Express / Fastify / Next.js API), который принимает запросы от фронтенда, проверяет аутентификацию и пересылает запросы к LLM.
     - Backend хранит API-ключи в безопасном хранилище (env/secret manager).
   - Proxy даёт нам контроль: rate limiting, caching, логирование и audit.

3) Ограничение доступа и аутентификация
   - Если приложение публичное — мы добавляем слой аутентификации (JWT / OAuth) перед разрешением вызовов, изменяющих мир (добавление событий, агентов).
   - Мы вводим разные роли: админ (полный доступ), зритель (только чтение), редактор (ограниченные действия).

4) Ограничение затрат и rate-limiting
   - На backend-проxy мы реализуем ограничения запросов на пользователя/ключ, чтобы избежать неожиданных расходов.
   - Мы вводим квоты и soft-fallback’ы: при превышении квоты возвращаем заранее подготовленные поведенческие шаблоны вместо реального LLM.

5) Валидирование и санитаризация ответов LLM
   - Все ответы LLM мы валидируем (код ожидает JSON-ответы). На proxy мы делаем дополнительную проверку структуры и ограничиваем размер полей.
   - Мы избегаем прямого инжектирования сырых ответов в UI без проверки.

6) Логи и приватность
   - Логи (`EventLog`) содержат текст взаимодействий. В продакшене мы храним логи отдельно и применяем retention-политику (например 30–90 дней).
   - Мы маскируем/удаляем чувствительные данные из логов перед длительным хранением.

7) Хранение состояния и резервные копии
   - Мы переносим `memories` и `relationships` в базу данных (Postgres / MongoDB). Настраиваем регулярные бэкапы и миграции схемы.

8) Безопасность деплоя и CI/CD
   - Секреты мы храним в secret manager (GitHub Actions Secrets, GitLab CI/CD, AWS Secrets Manager и т.д.).
   - CI билд не должен выводить секреты в логи.


Проектный план (milestones)
1. MVP (локальная разработка)
   - Мы завершили настройку фронтенда и визуализации (`App.tsx`, `components/`).
   - Мы подключили один LLM-провайдер (локальный Ollama или OpenAI через env).
   - Подготовили документацию и `.env.example`.

2. Защищённый backend-proxy
   - Мы реализуем простое Express/Next.js proxy для LLM.
   - Выполним локальное тестирование и интеграцию с frontend.

3. Персистентность и бэкапы
   - Подключим БД для `memories` и `relationships`.
   - Реализуем API для сохранения/загрузки состояний симуляции.

4. Наблюдаемость и мониторинг
   - Добавим метрики latency, ошибок LLM и количества вызовов.

5. Горизонтальное масштабирование
   - Внедрим очередь задач и pool воркеров для вызовов LLM.
   - Горизонтально масштабируем бекенд и БД.

Дальнейшие планы масштабирования (технические детали)
- Воркеры и очередь: Redis / BullMQ или RabbitMQ для организации requests queue; воркеры обрабатывают запросы к LLM и пишут результаты в БД/канал связи.
- Бэкенд-проксирование: маршрутизация по моделям (дорогие запросы на GPT-4o-mini, быстрые/локальные — на Ollama).
- Кэширование: Redis для ответов на часто повторяющиеся промпты.
- WebSockets: заменить периодическое опрашивание на пуш-обновления (SSE/WebSocket) для реального времени.
- Контроль качества ответов: авто-тесты промптов, фильтрация токсичности и валидация JSON-схемы.

Дополнительные замечания по безопасности ключей
- В репозитории не хранить `.env` с ключами. Загружать ключи из отдельного локального файла `.env.local`, который должен быть в `.gitignore`.
- На CI хранить ключи в секретах проекта, на хостинге — в manager'е секретов платформы.
- Локально для тестов можно использовать mock-режим (service возвращает заготовленный JSON), чтобы разработчики не работали с живыми ключами.

Готовность и проверки
- После внедрения proxy и secret management — провести security review, pentest основных эндпоинтов и проверку логов на утечки ключей.

Контактные рекомендации для команды
- Документировать все изменения в `services/` и промпты, хранить примеры ответов LLM для regression тестов.


